"""
Explainable AI (XAI) Module
Uses LIME for image explanations and SHAP for tabular explanations
NO TRAINING NEEDED - Uses pre-trained explainability libraries
"""

import logging
from typing import Dict, List, Any
import numpy as np
from PIL import Image

logger = logging.getLogger(__name__)

# Try to import LIME
try:
    from lime import lime_image
    from lime.wrappers.scikit_image import SegmentationAlgorithm
    LIME_AVAILABLE = True
except ImportError:
    LIME_AVAILABLE = False
    logger.warning("âš ï¸ LIME not installed. Install with: pip install lime")

# Try to import SHAP
try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    logger.warning("âš ï¸ SHAP not installed. Install with: pip install shap")


class Explainer:
    """
    Explainable AI module for agricultural predictions
    Provides explanations for model decisions
    """
    
    def __init__(self):
        """Initialize explainability module"""
        logger.info("ðŸ” Initializing Explainability Module...")
        
        self.lime_available = LIME_AVAILABLE
        self.shap_available = SHAP_AVAILABLE
        
        if self.lime_available:
            logger.info("âœ… LIME available for image explanations")
        else:
            logger.warning("âš ï¸ LIME not available")
        
        if self.shap_available:
            logger.info("âœ… SHAP available for tabular explanations")
        else:
            logger.warning("âš ï¸ SHAP not available")
    
    # ============================================================================
    # IMAGE EXPLANATION (LIME)
    # ============================================================================
    
    def explain_image_prediction(self, image: Image.Image, model: Any, top_labels: int = 3) -> Dict:
        """
        Explain disease detection prediction using LIME
        
        Args:
            image: PIL Image
            model: Disease detection model
            top_labels: Number of top predictions to explain
        
        Returns:
            Dictionary with explanation details
        """
        try:
            if not self.lime_available:
                return self._fallback_image_explanation(image)
            
            logger.info("ðŸ” Generating LIME explanation for image...")
            
            # Convert image to numpy array
            img_array = np.array(image)
            
            # Initialize LIME explainer
            explainer = lime_image.LimeImageExplainer()
            
            # Create prediction function wrapper
            def predict_fn(images):
                """Wrapper for model prediction"""
                results = []
                for img in images:
                    pil_img = Image.fromarray(img.astype('uint8'))
                    pred = model.predict(pil_img)
                    # Return probability distribution
                    # For now, use confidence as placeholder
                    conf = pred.get('confidence_score', 0.5)
                    probs = np.array([1 - conf, conf])
                    results.append(probs)
                return np.array(results)
            
            # Generate explanation
            explanation = explainer.explain_instance(
                img_array,
                predict_fn,
                top_labels=top_labels,
                hide_color=0,
                num_samples=1000,
                segmentation_fn=SegmentationAlgorithm('quickshift', kernel_size=4, max_dist=200, ratio=0.2)
            )
            
            # Get the top prediction
            top_label = explanation.top_labels[0]
            
            # Get positive and negative regions
            temp_positive, mask_positive = explanation.get_image_and_mask(
                top_label,
                positive_only=True,
                num_features=5,
                hide_rest=False
            )
            
            temp_negative, mask_negative = explanation.get_image_and_mask(
                top_label,
                positive_only=False,
                num_features=5,
                hide_rest=False
            )
            
            # Extract feature importance
            local_exp = explanation.local_exp[top_label]
            feature_importance = [
                {
                    "segment": int(segment),
                    "weight": float(weight)
                }
                for segment, weight in local_exp[:10]
            ]
            
            result = {
                "explanation_type": "LIME",
                "status": "success",
                "top_label": int(top_label),
                "feature_importance": feature_importance,
                "num_features_analyzed": len(local_exp),
                "interpretation": self._interpret_lime_results(feature_importance),
                "summary": "The highlighted regions show which parts of the leaf image most influenced the disease prediction.",
                "positive_regions": "Green areas indicate features supporting the prediction",
                "negative_regions": "Red areas indicate features contradicting the prediction"
            }
            
            logger.info("âœ… LIME explanation generated")
            return result
            
        except Exception as e:
            logger.error(f"âŒ LIME explanation error: {str(e)}")
            return self._fallback_image_explanation(image)
    
    def _interpret_lime_results(self, feature_importance: List[Dict]) -> str:
        """Interpret LIME feature importance"""
        
        positive_features = [f for f in feature_importance if f['weight'] > 0]
        negative_features = [f for f in feature_importance if f['weight'] < 0]
        
        interpretation = []
        
        if positive_features:
            interpretation.append(
                f"âœ… {len(positive_features)} image segments strongly support the disease prediction"
            )
        
        if negative_features:
            interpretation.append(
                f"âš ï¸ {len(negative_features)} image segments contradict the prediction (may indicate uncertainty)"
            )
        
        # Dominant features
        top_feature = feature_importance[0] if feature_importance else None
        if top_feature:
            if top_feature['weight'] > 0:
                interpretation.append(
                    f"ðŸ” The most influential region has {abs(top_feature['weight']):.2%} impact on the decision"
                )
        
        return " | ".join(interpretation) if interpretation else "Analysis complete"
    
    def _fallback_image_explanation(self, image: Image.Image) -> Dict:
        """Fallback explanation when LIME is not available"""
        return {
            "explanation_type": "Basic",
            "status": "limited",
            "message": "LIME not available. Install with: pip install lime",
            "summary": "The model analyzed the entire leaf image for disease symptoms.",
            "interpretation": "Key areas: leaf discoloration, spots, texture abnormalities",
            "recommendation": "Install LIME for detailed visual explanations: pip install lime"
        }
    
    # ============================================================================
    # TABULAR EXPLANATION (SHAP / Rule-based)
    # ============================================================================
    
    def explain_tabular_prediction(self, 
                                   features: List[float],
                                   feature_names: List[str],
                                   model_type: str = "soil") -> Dict:
        """
        Explain tabular model predictions
        
        Args:
            features: Feature values
            feature_names: Names of features
            model_type: Type of model ('soil', 'yield', 'pest')
        
        Returns:
            Dictionary with feature importance explanation
        """
        try:
            logger.info(f"ðŸ” Generating explanation for {model_type} prediction...")
            
            # Since most of our models are rule-based, we use custom logic
            if model_type == "soil":
                return self._explain_soil_prediction(features, feature_names)
            elif model_type == "yield":
                return self._explain_yield_prediction(features, feature_names)
            elif model_type == "pest":
                return self._explain_pest_prediction(features, feature_names)
            else:
                return self._generic_tabular_explanation(features, feature_names)
                
        except Exception as e:
            logger.error(f"âŒ Tabular explanation error: {str(e)}")
            return self._generic_tabular_explanation(features, feature_names)
    
    def _explain_soil_prediction(self, features: List[float], feature_names: List[str]) -> Dict:
        """Explain soil health prediction"""
        
        # Expected features: [pH, Nitrogen, Phosphorus, Potassium, Organic_Carbon, Moisture]
        feature_dict = dict(zip(feature_names, features))
        
        explanations = []
        feature_impacts = []
        
        # pH analysis
        if 'ph' in feature_dict or 'pH' in feature_dict:
            ph = feature_dict.get('ph', feature_dict.get('pH', 7))
            if 6.0 <= ph <= 7.5:
                impact = "positive"
                explanation = f"âœ… pH ({ph:.1f}) is optimal for nutrient availability"
            elif ph < 6.0:
                impact = "negative"
                explanation = f"ðŸ”´ pH ({ph:.1f}) is too acidic, limiting nutrient uptake"
            else:
                impact = "negative"
                explanation = f"ðŸ”´ pH ({ph:.1f}) is too alkaline, reducing nutrient availability"
            
            explanations.append(explanation)
            feature_impacts.append({
                "feature": "pH",
                "value": round(ph, 2),
                "impact": impact,
                "importance": 0.25
            })
        
        # Nitrogen analysis
        if 'nitrogen' in feature_dict or 'N' in feature_dict:
            n = feature_dict.get('nitrogen', feature_dict.get('N', 70))
            if 40 <= n <= 100:
                impact = "positive"
                explanation = f"âœ… Nitrogen ({n:.1f} kg/ha) is adequate"
            elif n < 40:
                impact = "negative"
                explanation = f"ðŸ”´ Nitrogen ({n:.1f} kg/ha) is deficient - limits growth"
            else:
                impact = "warning"
                explanation = f"âš ï¸ Nitrogen ({n:.1f} kg/ha) is excessive - risk of leaching"
            
            explanations.append(explanation)
            feature_impacts.append({
                "feature": "Nitrogen",
                "value": round(n, 2),
                "impact": impact,
                "importance": 0.30
            })
        
        # Phosphorus analysis
        if 'phosphorus' in feature_dict or 'P' in feature_dict:
            p = feature_dict.get('phosphorus', feature_dict.get('P', 25))
            if 15 <= p <= 40:
                impact = "positive"
                explanation = f"âœ… Phosphorus ({p:.1f} kg/ha) is good"
            elif p < 15:
                impact = "negative"
                explanation = f"ðŸ”´ Phosphorus ({p:.1f} kg/ha) is low - affects root development"
            else:
                impact = "warning"
                explanation = f"âš ï¸ Phosphorus ({p:.1f} kg/ha) is high"
            
            explanations.append(explanation)
            feature_impacts.append({
                "feature": "Phosphorus",
                "value": round(p, 2),
                "impact": impact,
                "importance": 0.20
            })
        
        # Potassium analysis
        if 'potassium' in feature_dict or 'K' in feature_dict:
            k = feature_dict.get('potassium', feature_dict.get('K', 150))
            if 100 <= k <= 250:
                impact = "positive"
                explanation = f"âœ… Potassium ({k:.1f} kg/ha) is sufficient"
            elif k < 100:
                impact = "negative"
                explanation = f"ðŸ”´ Potassium ({k:.1f} kg/ha) is deficient - affects fruit quality"
            else:
                impact = "warning"
                explanation = f"âš ï¸ Potassium ({k:.1f} kg/ha) is excessive"
            
            explanations.append(explanation)
            feature_impacts.append({
                "feature": "Potassium",
                "value": round(k, 2),
                "impact": impact,
                "importance": 0.25
            })
        
        result = {
            "explanation_type": "Rule-Based Feature Analysis",
            "model": "Soil Health Analyzer",
            "status": "success",
            "feature_importance": feature_impacts,
            "detailed_explanations": explanations,
            "summary": self._generate_soil_summary(feature_impacts),
            "key_factors": [f for f in feature_impacts if f['impact'] == 'negative'][:3],
            "methodology": "Analysis based on FAO soil fertility guidelines"
        }
        
        logger.info("âœ… Soil prediction explained")
        return result
    
    def _generate_soil_summary(self, feature_impacts: List[Dict]) -> str:
        """Generate summary of soil analysis"""
        
        positive = len([f for f in feature_impacts if f['impact'] == 'positive'])
        negative = len([f for f in feature_impacts if f['impact'] == 'negative'])
        warning = len([f for f in feature_impacts if f['impact'] == 'warning'])
        
        if negative >= 2:
            return f"Critical: {negative} parameters are deficient, requiring immediate attention"
        elif negative == 1:
            return f"Moderate: 1 parameter needs correction. {positive} parameters are optimal"
        elif warning > 0:
            return f"Good overall, but monitor {warning} parameter(s) for potential issues"
        else:
            return f"Excellent: All {positive} soil parameters are within optimal ranges"
    
    def _explain_yield_prediction(self, features: List[float], feature_names: List[str]) -> Dict:
        """Explain yield prediction"""
        
        feature_dict = dict(zip(feature_names, features))
        
        explanations = [
            "Yield prediction is based on historical trends and current conditions",
            "Key factors: Historical yield patterns, weather conditions, soil health",
            "Time-series analysis identifies seasonal patterns and growth trends"
        ]
        
        feature_impacts = []
        
        # If historical average is provided
        if 'historical_average' in feature_dict:
            avg = feature_dict['historical_average']
            feature_impacts.append({
                "feature": "Historical Average Yield",
                "value": round(avg, 2),
                "impact": "baseline",
                "importance": 0.40
            })
            explanations.append(f"ðŸ“Š Historical baseline: {avg:.2f} kg/ha")
        
        # If growth rate is provided
        if 'growth_rate' in feature_dict:
            rate = feature_dict['growth_rate']
            impact = "positive" if rate > 0 else "negative"
            feature_impacts.append({
                "feature": "Growth Rate",
                "value": round(rate, 2),
                "impact": impact,
                "importance": 0.30
            })
            explanations.append(f"ðŸ“ˆ Trend: {rate:+.2f}% per season")
        
        return {
            "explanation_type": "Time-Series Analysis",
            "model": "Yield Predictor",
            "status": "success",
            "feature_importance": feature_impacts,
            "detailed_explanations": explanations,
            "summary": "Prediction combines historical patterns with current seasonal factors",
            "methodology": "Prophet time-series forecasting with trend analysis"
        }
    
    def _explain_pest_prediction(self, features: List[float], feature_names: List[str]) -> Dict:
        """Explain pest risk prediction"""
        
        feature_dict = dict(zip(feature_names, features))
        
        explanations = []
        feature_impacts = []
        
        # Temperature impact
        if 'temperature' in feature_dict:
            temp = feature_dict['temperature']
            if 25 <= temp <= 32:
                impact = "negative"  # Increases pest risk
                explanation = f"ðŸ”´ Temperature ({temp}Â°C) is optimal for pest activity"
                importance = 0.30
            else:
                impact = "positive"  # Reduces pest risk
                explanation = f"âœ… Temperature ({temp}Â°C) is suboptimal for pests"
                importance = 0.15
            
            explanations.append(explanation)
            feature_impacts.append({
                "feature": "Temperature",
                "value": round(temp, 2),
                "impact": impact,
                "importance": importance
            })
        
        # Humidity impact
        if 'humidity' in feature_dict:
            humidity = feature_dict['humidity']
            if humidity > 80:
                impact = "negative"
                explanation = f"ðŸ”´ High humidity ({humidity}%) favors pest reproduction"
                importance = 0.35
            elif humidity > 60:
                impact = "warning"
                explanation = f"âš ï¸ Moderate humidity ({humidity}%) supports pest activity"
                importance = 0.20
            else:
                impact = "positive"
                explanation = f"âœ… Low humidity ({humidity}%) discourages pests"
                importance = 0.10
            
            explanations.append(explanation)
            feature_impacts.append({
                "feature": "Humidity",
                "value": round(humidity, 2),
                "impact": impact,
                "importance": importance
            })
        
        # Rainfall impact
        if 'rainfall' in feature_dict:
            rain = feature_dict['rainfall']
            if rain > 50:
                impact = "negative"
                explanation = f"ðŸ”´ Heavy rainfall ({rain}mm) increases disease vectors"
                importance = 0.25
            else:
                impact = "neutral"
                explanation = f"Rainfall ({rain}mm) has moderate impact"
                importance = 0.10
            
            explanations.append(explanation)
            feature_impacts.append({
                "feature": "Rainfall (7 days)",
                "value": round(rain, 2),
                "impact": impact,
                "importance": importance
            })
        
        return {
            "explanation_type": "Environmental Risk Analysis",
            "model": "Pest Risk Assessor",
            "status": "success",
            "feature_importance": feature_impacts,
            "detailed_explanations": explanations,
            "summary": "Risk level determined by combination of temperature, humidity, and rainfall",
            "key_factors": [f for f in feature_impacts if f['importance'] > 0.25],
            "methodology": "Rule-based entomological risk assessment"
        }
    
    def _generic_tabular_explanation(self, features: List[float], feature_names: List[str]) -> Dict:
        """Generic fallback explanation"""
        
        feature_impacts = [
            {
                "feature": name,
                "value": round(value, 2),
                "impact": "analyzed",
                "importance": 1.0 / len(features)
            }
            for name, value in zip(feature_names, features)
        ]
        
        return {
            "explanation_type": "Basic Feature Analysis",
            "status": "success",
            "feature_importance": feature_impacts,
            "summary": f"Analyzed {len(features)} features for prediction",
            "note": "Install SHAP for advanced feature importance: pip install shap"
        }
    
    # ============================================================================
    # UTILITY METHODS
    # ============================================================================
    
    def get_capabilities(self) -> Dict:
        """Get explainability capabilities"""
        return {
            "lime_available": self.lime_available,
            "shap_available": self.shap_available,
            "image_explanation": self.lime_available,
            "tabular_explanation": True,  # Rule-based always available
            "supported_models": ["disease_detection", "soil_analysis", "yield_prediction", "pest_risk"]
        }


# Singleton instance
_explainer_instance = None


def get_explainer() -> Explainer:
    """Get singleton instance of Explainer"""
    global _explainer_instance
    if _explainer_instance is None:
        _explainer_instance = Explainer()
    return _explainer_instance


if __name__ == "__main__":
    # Test explainability
    print("Testing Explainability Module...")
    
    explainer = Explainer()
    print(f"\nCapabilities: {explainer.get_capabilities()}")
    
    # Test tabular explanation
    print("\nðŸ“Š Testing Soil Explanation:")
    soil_features = [6.5, 70, 25, 150, 1.0, 0.5]
    soil_names = ['pH', 'nitrogen', 'phosphorus', 'potassium', 'organic_carbon', 'moisture']
    
    explanation = explainer.explain_tabular_prediction(soil_features, soil_names, "soil")
    print(f"Summary: {explanation['summary']}")
    print(f"Features analyzed: {len(explanation['feature_importance'])}")
    
    print("\nâœ… Explainability Module test completed!")